{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature concatenation for Yes We Can dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature concatenation for the Yes We Can dataset where the numerical structural features are concatenated with the BERT embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting pandas==1.3.4\n",
      "  Downloading pandas-1.3.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.5 MB 14.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.8/site-packages (from pandas==1.3.4) (1.21.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas==1.3.4) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas==1.3.4) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas==1.3.4) (1.16.0)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.3.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting transformers==4.12.5\n",
      "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 19.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers==4.12.5) (21.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers==4.12.5) (5.4.1)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
      "\u001b[K     |████████████████████████████████| 67 kB 41.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers==4.12.5) (1.21.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers==4.12.5) (2.26.0)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 28.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers==4.12.5) (3.3.0)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.8/site-packages (from transformers==4.12.5) (0.0.46)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers==4.12.5) (2021.10.8)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers==4.12.5) (4.62.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.12.5) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers==4.12.5) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.12.5) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.12.5) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.12.5) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.12.5) (2.0.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers==4.12.5) (8.0.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers==4.12.5) (1.16.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers==4.12.5) (1.1.0)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.4.0 tokenizers-0.10.3 transformers-4.12.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting datasets==1.15.1\n",
      "  Downloading datasets-1.15.1-py3-none-any.whl (290 kB)\n",
      "\u001b[K     |████████████████████████████████| 290 kB 22.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (1.21.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (4.62.3)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 29.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[K     |████████████████████████████████| 212 kB 27.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec[http]>=2021.05.0\n",
      "  Downloading fsspec-2022.2.0-py3-none-any.whl (134 kB)\n",
      "\u001b[K     |████████████████████████████████| 134 kB 31.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (2.26.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (0.4.0)\n",
      "Collecting pyarrow!=4.0.0,>=1.0.0\n",
      "  Downloading pyarrow-7.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 26.7 MB 24.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multiprocess\n",
      "  Downloading multiprocess-0.70.12.2-py38-none-any.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 29.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dill\n",
      "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[K     |████████████████████████████████| 86 kB 33.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (21.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets==1.15.1) (1.3.4)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.15.1) (5.4.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.15.1) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets==1.15.1) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets==1.15.1) (2.4.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==1.15.1) (2.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==1.15.1) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==1.15.1) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==1.15.1) (3.1)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
      "\u001b[K     |████████████████████████████████| 158 kB 27.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (308 kB)\n",
      "\u001b[K     |████████████████████████████████| 308 kB 31.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==1.15.1) (21.2.0)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
      "\u001b[K     |████████████████████████████████| 121 kB 5.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets==1.15.1) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets==1.15.1) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets==1.15.1) (1.16.0)\n",
      "Installing collected packages: multidict, frozenlist, yarl, async-timeout, aiosignal, fsspec, dill, aiohttp, xxhash, pyarrow, multiprocess, datasets\n",
      "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 datasets-1.15.1 dill-0.3.4 frozenlist-1.3.0 fsspec-2022.2.0 multidict-6.0.2 multiprocess-0.70.12.2 pyarrow-7.0.0 xxhash-3.0.0 yarl-1.7.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-7.7.0-py2.py3-none-any.whl (123 kB)\n",
      "\u001b[K     |████████████████████████████████| 123 kB 21.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: nbformat>=4.2.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.1.3)\n",
      "Collecting widgetsnbextension~=3.6.0\n",
      "  Downloading widgetsnbextension-3.6.0-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 26.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (0.2.0)\n",
      "Collecting jupyterlab-widgets>=1.0.0\n",
      "  Downloading jupyterlab_widgets-1.1.0-py3-none-any.whl (245 kB)\n",
      "\u001b[K     |████████████████████████████████| 245 kB 22.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: ipython>=4.0.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (7.28.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (6.4.1)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (7.0.6)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (3.0.20)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.18.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (58.2.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.2)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (4.8.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (0.3)\n",
      "Requirement already satisfied: pyzmq>=13 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (22.3.0)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (1.5.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets) (4.0.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (21.2.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.18.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.1->jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.8/site-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.4.1)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.2.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.12.1)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.11.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.1)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.1.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.20)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.1.0)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.4)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.0)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.4.7)\n",
      "Installing collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-7.7.0 jupyterlab-widgets-1.1.0 widgetsnbextension-3.6.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas==1.3.4\n",
    "!pip install transformers==4.12.5\n",
    "!pip install datasets==1.15.1\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "import transformers\n",
    "from transformers import Trainer\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel, BertPreTrainedModel\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.data.data_collator import DataCollatorWithPadding\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from datasets import ClassLabel\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files and folders path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your paths go here:\n",
    "\n",
    "DATA_FILE = '../datasets/dataset_yes_we_can_icann_v2.pt'\n",
    "RESULTS_FOLDER = '../results/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.load(DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['split', 'text', 'labels', 'full_sentence', 'context_and_full_sentence', 'context_full_sentence_structural_fts_as_txt', 'context_full_sentence_structural_fts_as_txt_combined', 'feature_tensor'],\n",
       "        num_rows: 10447\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['split', 'text', 'labels', 'full_sentence', 'context_and_full_sentence', 'context_full_sentence_structural_fts_as_txt', 'context_full_sentence_structural_fts_as_txt_combined', 'feature_tensor'],\n",
       "        num_rows: 6567\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['split', 'text', 'labels', 'full_sentence', 'context_and_full_sentence', 'context_full_sentence_structural_fts_as_txt', 'context_full_sentence_structural_fts_as_txt_combined', 'feature_tensor'],\n",
       "        num_rows: 5226\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b712c81a4b244f5ad85d1f2b2565440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10447 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "926e99b4d9864c6bb82b7dd774dd22ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6567 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96139814a69044b1aad0f5f50fd8bdbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5226 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def correct_feature_tensor(example):\n",
    "    \n",
    "    a = example['feature_tensor']\n",
    "    b = np.fromstring(a[1:-1], sep=\" \")\n",
    "    c = torch.tensor(b)\n",
    "    \n",
    "    example['feature_tensor'] = c\n",
    "    \n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(correct_feature_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c851775d13426b9d4b2ff85a484806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "072da6b26a2b4d8da341a72d0963a1eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "530760a10b1b44879bf9052b2935e319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519a967b59004c3591ac087de5540dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = set(dataset['train']['labels'])\n",
    "label_nb = len(label_names)\n",
    "labels = ClassLabel(num_classes=label_nb, names=label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassLabel(num_classes=2, names={'Claim', 'Premise'}, names_file=None, id=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    tokens = tokenizer(batch['context_and_full_sentence'], truncation=True, padding=True, max_length=512)\n",
    "    tokens['labels'] = labels.str2int(batch['labels'])\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de902b97ed94bdf8aab99f83b382f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d440df6b58f9442ea5529362637075e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371bed13a14a4fe18be43f9137d718c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels', 'feature_tensor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train'].shuffle(seed=42)\n",
    "test_dataset = dataset['test'].shuffle(seed=42)\n",
    "\n",
    "train_val_datasets = dataset['train'].train_test_split(train_size=0.8)\n",
    "train_dataset = train_val_datasets['train']\n",
    "val_dataset = train_val_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_d = {}\n",
    "dataset_d['train'] = train_dataset\n",
    "dataset_d['test'] = test_dataset\n",
    "dataset_d['val'] = val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LABELS = labels.num_classes\n",
    "NUM_FEATURES = dataset_d['train']['feature_tensor'].shape[1]\n",
    "BATCH_SIZE = 16\n",
    "NB_EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBertForSequenceClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_features):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.num_features = num_features # *** MODIF ***\n",
    "        \n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size + num_features, config.num_labels) # *** MODIF ***\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        feature_tensor=None\n",
    "    ):\n",
    "        \n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`, defaults to :obj:`None`):\n",
    "            Labels for computing the sequence classification/regression loss.\n",
    "            Indices should be in :obj:`[0, ..., config.num_labels - 1]`.\n",
    "            If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
    "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "\n",
    "    Returns:\n",
    "        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.BertConfig`) and inputs:\n",
    "        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`label` is provided):\n",
    "            Classification (or regression if config.num_labels==1) loss.\n",
    "        logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, config.num_labels)`):\n",
    "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
    "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_hidden_states=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
    "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_attentions=True``):\n",
    "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\n",
    "            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        from transformers import BertTokenizer, BertForSequenceClassification\n",
    "        import torch\n",
    "\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "\n",
    "        loss, logits = outputs[:2]\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = torch.cat([pooled_output, feature_tensor], dim=1)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "                \n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:] \n",
    "\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "            \n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "092c5f8c34774533a327d31c59fdb090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing CustomBertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing CustomBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CustomBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CustomBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CustomBertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=775, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CustomBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n",
    "                                                            num_labels=NUM_LABELS, \n",
    "                                                            num_features=NUM_FEATURES)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs[1]#.get('logits')\n",
    "        loss_fct = nn.CrossEntropyLoss()#(weight=class_weights)\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "020213ffa37f41f49aa9100977758e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = load_metric('f1')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    return metric.compute(predictions=predictions, references=labels, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \n",
    "    # output\n",
    "    output_dir=RESULTS_FOLDER,          \n",
    "    \n",
    "    # params\n",
    "    num_train_epochs=NB_EPOCHS,               # nb of epochs\n",
    "    per_device_train_batch_size=BATCH_SIZE,   # batch size per device during training\n",
    "    per_device_eval_batch_size=BATCH_SIZE,    # cf. paper Sun et al.\n",
    "    learning_rate=5e-5,#2e-5,                 # cf. paper Sun et al.\n",
    "#     warmup_steps=500,                         # number of warmup steps for learning rate scheduler\n",
    "    warmup_ratio=0.1,                         # cf. paper Sun et al.\n",
    "    weight_decay=0.01,                        # strength of weight decay\n",
    "    \n",
    "    # eval\n",
    "    evaluation_strategy=\"steps\",              # cf. paper Sun et al.\n",
    "    eval_steps=20,                            # cf. paper Sun et al.\n",
    "    \n",
    "    # log\n",
    "    logging_dir=RESULTS_FOLDER+'logs',  \n",
    "    logging_strategy='steps',\n",
    "    logging_steps=20,\n",
    "    \n",
    "    # save\n",
    "    save_strategy='steps',\n",
    "    save_total_limit=2,\n",
    "    # save_steps=20, # default 500\n",
    "    load_best_model_at_end=True,              # cf. paper Sun et al.\n",
    "    # metric_for_best_model='eval_loss'\n",
    "    metric_for_best_model='f1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CustomTrainer( # Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    "    # callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running training *****\n",
      "  Num examples = 8357\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1569\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1569' max='1569' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1569/1569 29:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.785800</td>\n",
       "      <td>0.700918</td>\n",
       "      <td>0.412064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.697100</td>\n",
       "      <td>0.685404</td>\n",
       "      <td>0.359874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.677000</td>\n",
       "      <td>0.686248</td>\n",
       "      <td>0.359614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.696700</td>\n",
       "      <td>0.662193</td>\n",
       "      <td>0.464391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.619600</td>\n",
       "      <td>0.717946</td>\n",
       "      <td>0.401767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.642000</td>\n",
       "      <td>0.605594</td>\n",
       "      <td>0.679125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.567100</td>\n",
       "      <td>0.675074</td>\n",
       "      <td>0.622036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.621900</td>\n",
       "      <td>0.558783</td>\n",
       "      <td>0.714146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.658400</td>\n",
       "      <td>0.575489</td>\n",
       "      <td>0.691168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.618100</td>\n",
       "      <td>0.545186</td>\n",
       "      <td>0.730100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.606500</td>\n",
       "      <td>0.558427</td>\n",
       "      <td>0.702203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.594800</td>\n",
       "      <td>0.553039</td>\n",
       "      <td>0.723372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.521800</td>\n",
       "      <td>0.584073</td>\n",
       "      <td>0.733344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.554500</td>\n",
       "      <td>0.535371</td>\n",
       "      <td>0.746025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.560800</td>\n",
       "      <td>0.566553</td>\n",
       "      <td>0.693868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.593000</td>\n",
       "      <td>0.581976</td>\n",
       "      <td>0.677687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.539700</td>\n",
       "      <td>0.602126</td>\n",
       "      <td>0.711429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.581700</td>\n",
       "      <td>0.521701</td>\n",
       "      <td>0.742754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.504700</td>\n",
       "      <td>0.552731</td>\n",
       "      <td>0.741475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.528000</td>\n",
       "      <td>0.517039</td>\n",
       "      <td>0.746845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.519800</td>\n",
       "      <td>0.516650</td>\n",
       "      <td>0.738309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.573000</td>\n",
       "      <td>0.541272</td>\n",
       "      <td>0.697135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.531600</td>\n",
       "      <td>0.539815</td>\n",
       "      <td>0.720993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.504000</td>\n",
       "      <td>0.501798</td>\n",
       "      <td>0.748536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.518500</td>\n",
       "      <td>0.533581</td>\n",
       "      <td>0.730489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.555400</td>\n",
       "      <td>0.528154</td>\n",
       "      <td>0.732522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.467500</td>\n",
       "      <td>0.525404</td>\n",
       "      <td>0.755740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.406800</td>\n",
       "      <td>0.538413</td>\n",
       "      <td>0.756720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.415500</td>\n",
       "      <td>0.526192</td>\n",
       "      <td>0.749145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.417200</td>\n",
       "      <td>0.528338</td>\n",
       "      <td>0.752937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.428400</td>\n",
       "      <td>0.533797</td>\n",
       "      <td>0.753776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.377400</td>\n",
       "      <td>0.562268</td>\n",
       "      <td>0.756982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.493600</td>\n",
       "      <td>0.546846</td>\n",
       "      <td>0.731790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.446600</td>\n",
       "      <td>0.540452</td>\n",
       "      <td>0.754773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.398900</td>\n",
       "      <td>0.530971</td>\n",
       "      <td>0.755936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.436100</td>\n",
       "      <td>0.505681</td>\n",
       "      <td>0.760516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.434700</td>\n",
       "      <td>0.519952</td>\n",
       "      <td>0.754550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.449200</td>\n",
       "      <td>0.537676</td>\n",
       "      <td>0.742298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.372600</td>\n",
       "      <td>0.530812</td>\n",
       "      <td>0.754451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.402100</td>\n",
       "      <td>0.574057</td>\n",
       "      <td>0.739992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.457300</td>\n",
       "      <td>0.516024</td>\n",
       "      <td>0.747997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.422500</td>\n",
       "      <td>0.549400</td>\n",
       "      <td>0.746822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.439800</td>\n",
       "      <td>0.518423</td>\n",
       "      <td>0.744312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.449600</td>\n",
       "      <td>0.528367</td>\n",
       "      <td>0.737499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.501600</td>\n",
       "      <td>0.515862</td>\n",
       "      <td>0.744595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.413300</td>\n",
       "      <td>0.544143</td>\n",
       "      <td>0.748605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.448800</td>\n",
       "      <td>0.497195</td>\n",
       "      <td>0.751866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.368900</td>\n",
       "      <td>0.521333</td>\n",
       "      <td>0.751313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.467400</td>\n",
       "      <td>0.513983</td>\n",
       "      <td>0.750492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.374800</td>\n",
       "      <td>0.571678</td>\n",
       "      <td>0.709119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.460400</td>\n",
       "      <td>0.509217</td>\n",
       "      <td>0.751685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.369900</td>\n",
       "      <td>0.522568</td>\n",
       "      <td>0.756003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.370700</td>\n",
       "      <td>0.547297</td>\n",
       "      <td>0.754705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.218500</td>\n",
       "      <td>0.627912</td>\n",
       "      <td>0.752661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.242200</td>\n",
       "      <td>0.691269</td>\n",
       "      <td>0.755630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.228800</td>\n",
       "      <td>0.690558</td>\n",
       "      <td>0.755895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.221700</td>\n",
       "      <td>0.664436</td>\n",
       "      <td>0.750803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.220500</td>\n",
       "      <td>0.672549</td>\n",
       "      <td>0.744850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.234800</td>\n",
       "      <td>0.754491</td>\n",
       "      <td>0.747116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.215500</td>\n",
       "      <td>0.694134</td>\n",
       "      <td>0.747369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.170600</td>\n",
       "      <td>0.739482</td>\n",
       "      <td>0.756381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.235000</td>\n",
       "      <td>0.784659</td>\n",
       "      <td>0.750061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.228900</td>\n",
       "      <td>0.778768</td>\n",
       "      <td>0.753916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.248700</td>\n",
       "      <td>0.725559</td>\n",
       "      <td>0.751410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.187000</td>\n",
       "      <td>0.790159</td>\n",
       "      <td>0.751111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.232200</td>\n",
       "      <td>0.752346</td>\n",
       "      <td>0.750010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.746202</td>\n",
       "      <td>0.752208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.268800</td>\n",
       "      <td>0.688150</td>\n",
       "      <td>0.750535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>0.267900</td>\n",
       "      <td>0.664275</td>\n",
       "      <td>0.744595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.158600</td>\n",
       "      <td>0.685544</td>\n",
       "      <td>0.748227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>0.194400</td>\n",
       "      <td>0.706216</td>\n",
       "      <td>0.748057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.244300</td>\n",
       "      <td>0.699231</td>\n",
       "      <td>0.747321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>0.239600</td>\n",
       "      <td>0.686473</td>\n",
       "      <td>0.748399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>0.203600</td>\n",
       "      <td>0.693388</td>\n",
       "      <td>0.747231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.216000</td>\n",
       "      <td>0.684761</td>\n",
       "      <td>0.747235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.258700</td>\n",
       "      <td>0.679515</td>\n",
       "      <td>0.746052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>0.196600</td>\n",
       "      <td>0.679448</td>\n",
       "      <td>0.746939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.212300</td>\n",
       "      <td>0.682001</td>\n",
       "      <td>0.746696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /notebooks/Results/bert_sequence_classification/checkpoint-500\n",
      "Configuration saved in /notebooks/Results/bert_sequence_classification/checkpoint-500/config.json\n",
      "Model weights saved in /notebooks/Results/bert_sequence_classification/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /notebooks/Results/bert_sequence_classification/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /notebooks/Results/bert_sequence_classification/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /notebooks/Results/bert_sequence_classification/checkpoint-1000\n",
      "Configuration saved in /notebooks/Results/bert_sequence_classification/checkpoint-1000/config.json\n",
      "Model weights saved in /notebooks/Results/bert_sequence_classification/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /notebooks/Results/bert_sequence_classification/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /notebooks/Results/bert_sequence_classification/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [/notebooks/Results/bert_sequence_classification/checkpoint-1500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /notebooks/Results/bert_sequence_classification/checkpoint-1500\n",
      "Configuration saved in /notebooks/Results/bert_sequence_classification/checkpoint-1500/config.json\n",
      "Model weights saved in /notebooks/Results/bert_sequence_classification/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /notebooks/Results/bert_sequence_classification/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /notebooks/Results/bert_sequence_classification/checkpoint-1500/special_tokens_map.json\n",
      "Deleting older checkpoint [/notebooks/Results/bert_sequence_classification/checkpoint-500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2090\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /notebooks/Results/bert_sequence_classification/checkpoint-1500 (score: 0.7472346250184382).\n"
     ]
    }
   ],
   "source": [
    "results = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomBertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=775, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the test set  don't have a corresponding argument in `CustomBertForSequenceClassification.forward` and have been ignored: full_sentence, split, context_full_sentence_structural_fts_as_txt, context_and_full_sentence, text, context_full_sentence_structural_fts_as_txt_combined.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 6567\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='821' max='821' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [821/821 00:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_trainer = Trainer(model, data_collator=DataCollatorWithPadding(tokenizer))\n",
    "test_raw_preds, test_labels, _ = test_trainer.predict(test_dataset)\n",
    "test_preds = np.argmax(test_raw_preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Claim      0.666     0.734     0.698      3358\n",
      "     Premise      0.688     0.615     0.650      3209\n",
      "\n",
      "    accuracy                          0.676      6567\n",
      "   macro avg      0.677     0.674     0.674      6567\n",
      "weighted avg      0.677     0.676     0.675      6567\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_name = labels.int2str([0,1])\n",
    "print(classification_report(test_labels, test_preds, target_names=target_name, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
